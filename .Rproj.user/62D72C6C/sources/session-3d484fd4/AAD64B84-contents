---
title: "script"
author: "Chun-Chien Hsueh"
date: "2025-05-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Load required packages and dataset

```{r}
# Load required packages
  
library(here)
library(ds4ling)
library(tidyverse)
library(ggplot2)
library(foreign)
library(glmnet)
library(caret)
library(corrplot)
library(psych)
library(pROC)
library(stabs) 
library(janitor) 
library(randomForest)
library(signal) # FFT 
library(e1071)
library(nnet)
library(pROC)

# load data
data <- read.arff(here("data","eye_state.arff"))

# checking data structure
str(data)

# dealing with outliers
data <- data %>%
  mutate(across(where(is.numeric), ~ ifelse(. < 0 | . > 10000, NA, .))) %>%
  na.omit()
```


## 2. EDA

### 2.1 Description Statistics

```{r summary-stats}
summary_stats <- summary(data)

# Description Statistics（remove eyeDetection, since it is factor variable）
desc_stats <- data %>%
  select(-eyeDetection) %>%
  summarise_all(list(sd = sd)) %>%
  pivot_longer(everything(), names_to = c("Variable", "Statistic"), names_sep = "_")

# show results
summary_stats
desc_stats

```


```{r}
library(papaja)
# remove factor column
numeric_data <- data %>% select(where(is.numeric))

# Culculate description statistics
summary_table <- numeric_data %>%
  summarise(across(everything(), list(
    Min = ~min(.),
    Q1 = ~quantile(., 0.25),
    Median = ~median(.),
    Q3 = ~quantile(., 0.75),
    Max = ~max(.),
    SE = ~sd(.) / sqrt(length(.))
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(everything(),
               names_to = c("Variable", "Statistic"),
               names_sep = "_") %>%
  pivot_wider(names_from = Statistic, values_from = value) %>%
  arrange(Variable)

# output the table in APA format 
apa_table(summary_table,
          caption = "Five-number summaries and standard errors for each variable.",
          align = c("l", rep("r", 6)))
```


### 2.2 Visualize the variables

```{r hist-plots, fig.width=12, fig.height=10}
hist_plots <- data %>%
  select(-eyeDetection) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  theme_minimal() +
  ggtitle("Distribution of EEG Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

hist_plots
```

### 2.3 Correlation plot

```{r corr-plot}
cor_matrix <- cor(data %>% select(-eyeDetection))
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, addCoef.col = "black")
```

### 2.4 eyeDetection distribution plot

```{r eye-detection-plot, fig.width=6, fig.height=4}
eye_detection_plot <- ggplot(data, aes(x = as.factor(eyeDetection), fill = as.factor(eyeDetection))) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Eye Detection", x = "Eye Detection", y = "Count") +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"))

eye_detection_plot
```



## 3. Tidy Data

```{r tidy-data}
# Checking for missing value
missing_values <- colSums(is.na(data))
if (any(missing_values > 0)) {
  data <- na.omit(data) # remove missing value
}

# Make sure eyeDetection is factor
data$eyeDetection <- as.factor(data$eyeDetection)

# Show missing value
missing_values
```



## 4. Data pre-processing and Feature engineering

### 4.1 Scale and Moving Average denoising

```{r preprocess}
eeg_vars <- names(data)[names(data) != "eyeDetection"]
data_eeg <- data %>% select(all_of(eeg_vars))

data_eeg_scaled <- as.data.frame(scale(data_eeg))

window_size <- 5
data_smoothed <- data_eeg_scaled %>%
  mutate_all(~ stats::filter(., rep(1/window_size, window_size), sides = 1))

n_rows <- nrow(data_eeg_scaled)
valid_rows <- (window_size):n_rows
data_smoothed <- data_smoothed[valid_rows, ]
data_smoothed <- as.data.frame(data_smoothed)

data <- data[valid_rows, ]
data$eyeDetection <- data$eyeDetection[valid_rows]
data[eeg_vars] <- data_smoothed

str(data)
```

### 4.2 Frequency domain feature engineering (FFT to extract frequency band power)

```{r fft-features}
sampling_rate <- 128
window_size <- 128
step_size <- 16 
n <- nrow(data)
channels <- eeg_vars

delta_band <- c(0.5, 4)
theta_band <- c(4, 8)
alpha_band <- c(8, 13)
beta_band <- c(13, 30)
gamma_band <- c(30, 50)

feature_data <- data.frame()

for (start in seq(1, n - window_size + 1, by = step_size)) {
  end <- start + window_size - 1
  window_data <- data[start:end, channels]
  
  fft_features <- lapply(channels, function(ch) {
    signal <- window_data[[ch]]
    fft_result <- fft(signal)
    freq <- (0:(window_size-1)) * sampling_rate / window_size
    
    power <- abs(fft_result)^2 / window_size
    total_power <- sum(power, na.rm = TRUE) + 1e-10 # 避免除以 0
    
    delta_idx <- which(freq >= delta_band[1] & freq <= delta_band[2])
    theta_idx <- which(freq >= theta_band[1] & freq <= theta_band[2])
    alpha_idx <- which(freq >= alpha_band[1] & freq <= alpha_band[2])
    beta_idx <- which(freq >= beta_band[1] & freq <= beta_band[2])
    gamma_idx <- which(freq >= gamma_band[1] & freq <= gamma_band[2])
    
    delta_power <- sum(power[delta_idx], na.rm = TRUE) / total_power
    theta_power <- sum(power[theta_idx], na.rm = TRUE) / total_power
    alpha_power <- sum(power[alpha_idx], na.rm = TRUE) / total_power
    beta_power <- sum(power[beta_idx], na.rm = TRUE) / total_power
    gamma_power <- sum(power[gamma_idx], na.rm = TRUE) / total_power
    
    c(delta_power = delta_power, theta_power = theta_power, 
      alpha_power = alpha_power, beta_power = beta_power, 
      gamma_power = gamma_power)
  })
  
  fft_features_df <- as.data.frame(do.call(rbind, fft_features))
  colnames(fft_features_df) <- c("delta_power", "theta_power", "alpha_power", "beta_power", "gamma_power")
  fft_features_df$channel <- channels
  fft_features_df$window_start <- start
  
  feature_data <- rbind(feature_data, fft_features_df)
}

feature_data_wide <- feature_data %>%
  pivot_wider(names_from = channel, values_from = c("delta_power", "theta_power", "alpha_power", "beta_power", "gamma_power"),
              names_glue = "fft_{.value}_{channel}")

window_centers <- seq(1, n - window_size + 1, by = step_size) + (window_size / 2)
window_centers <- round(window_centers)
data_reduced <- data[window_centers, ]

if (nrow(data_reduced) != nrow(feature_data_wide)) {
  min_rows <- min(nrow(data_reduced), nrow(feature_data_wide))
  data_reduced <- data_reduced[1:min_rows, ]
  feature_data_wide <- feature_data_wide[1:min_rows, ]
}

data_reduced <- cbind(data_reduced, feature_data_wide)
```

### 4.3 Interaction feature generation (reducing interaction features)
Only biologically meaningful interaction features were retained and examined for correlations.

```{r interaction-features}
data_features <- data_reduced %>%
  mutate(
    AF_diff = AF3 - AF4,
    O_diff = O1 - O2,
    AF3_O1_interaction = AF3 * O1,
    alpha_O1_alpha_AF3_diff = fft_alpha_power_O1 - fft_alpha_power_AF3,
    beta_alpha_ratio_FC5 = fft_beta_power_FC5 / (fft_alpha_power_FC5 + 1e-10)
  ) %>%
  na.omit()

# Checking the correlation between features and target variable
cor_with_target <- data_features %>%
  select(-eyeDetection) %>%
  summarise_all(~ cor(., as.numeric(data_features$eyeDetection), use = "complete.obs")) %>%
  pivot_longer(everything(), names_to = "Feature", values_to = "Correlation")

# Checking whether generate cor_with_target successfully or not
print("Structure of cor_with_target:")
str(cor_with_target)

# Using dplyr::filter filter high correlation features
high_cor_features <- cor_with_target %>%
  dplyr::filter(abs(Correlation) > 0.9) %>%
  pull(Feature)

if (length(high_cor_features) > 0) {
  cat("Removing features with high correlation to target:", high_cor_features, "\n")
  data_features <- data_features %>% select(-all_of(high_cor_features))
} else {
  cat("No features with high correlation to target (abs(Correlation) > 0.9) found.\n")
}
```

### 4.4 Statistical feature generation (dynamic calculation)

```{r statistical-features}
stat_features <- data.frame()
for (start in seq(1, n - window_size + 1, by = step_size)) {
  end <- start + window_size - 1
  window_data <- data[start:end, channels]
  
  stat_window <- window_data %>%
    summarise_all(list(
      skewness = ~ psych::skew(., na.rm = TRUE),
      kurtosis = ~ psych::kurtosi(., na.rm = TRUE)
    )) %>%
    pivot_longer(everything(), names_to = c("Statistic", "Channel"), names_sep = "_") %>%
    pivot_wider(names_from = Statistic, values_from = value)
  
  stat_window$window_start <- start
  stat_features <- rbind(stat_features, stat_window)
}

# Check the structure of stat_features and feature_data
print("Structure of stat_features before filtering:")
str(stat_features)
print("Structure of feature_data:")
str(feature_data)

stat_features <- stat_features %>%
  dplyr::filter(window_start %in% feature_data$window_start) %>%
  dplyr::select(-window_start) %>%
  na.omit()

if (nrow(data_features) != nrow(stat_features)) {
  min_rows <- min(nrow(data_features), nrow(stat_features))
  data_features <- data_features[1:min_rows, ]
  stat_features <- stat_features[1:min_rows, ]
}

data_features <- cbind(data_features, stat_features[names(stat_features) != "Channel"])
```

### 4.5 Oversampling

```{r upsample}
set.seed(1754)
data_features <- data_features %>% janitor::clean_names()

data_upsampled <- upSample(x = data_features %>% select(-eye_detection),
                           y = data_features$eye_detection,
                           yname = "eye_detection")

data_features <- data_upsampled

table(data_features$eye_detection)
```

## 5. Devide the training data set and testing dataset

```{r train-test-split}
set.seed(1754)
trainIndex <- createDataPartition(data_features$eye_detection, p = 0.8, list = FALSE)
train_data <- data_features[trainIndex, ]
test_data <- data_features[-trainIndex, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
cat("Train set class distribution:\n")
print(table(train_data$eye_detection))
cat("Test set class distribution:\n")
print(table(test_data$eye_detection))
```

## 6. LASSO feature selection

Use lambda.1se, check multicollinearity

```{r lasso-stability, fig.width=6, fig.height=4}
# check and set working directory
if (!dir.exists("output")) {
  dir.create("output")
}
setwd("output")
cat("Current working directory:", getwd(), "\n")

# data
features <- train_data %>% dplyr::select(-eye_detection)
X <- as.matrix(features)
y <- train_data$eye_detection

# scale the features
X_scaled <- scale(X)

# LASSO modeling
set.seed(1754)
lasso_model <- cv.glmnet(X_scaled, y, family = "binomial", alpha = 1)
best_lambda <- lasso_model$lambda.1se

# Extract non-zero coefficients
lasso_coefs <- coef(lasso_model, s = "lambda.1se")
lasso_coef_df <- as.data.frame(as.matrix(lasso_coefs)) %>%
  tibble::rownames_to_column("Variable") %>%
  dplyr::filter(s1 != 0, Variable != "(Intercept)") %>%
  dplyr::rename(Coefficient = s1)

# show results
cat("LASSO Selected Variables:\n")
print(lasso_coef_df$Variable)

# Coefficient plot
lasso_coef_plot <- ggplot(lasso_coef_df, aes(x = reorder(Variable, abs(Coefficient)), y = Coefficient)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "LASSO Selected Variables Coefficients", x = "Variable", y = "Coefficient")+
  theme(
    axis.text.y = element_text(size = 4, hjust = 1, margin = unit(c(0, 5, 0, 0), "pt")), 
    axis.text.x = element_text(size = 10),
    plot.title = element_text(size = 12, hjust = 0.5),
    plot.margin = unit(c(10, 20, 10, 20), "pt") 
  )


best_lambda
print(lasso_coef_df)
lasso_coef_plot
```

## 8. Random Forest Model for Feature Selection

```{r}

  train_data_selected <- train_data %>% select(all_of(c(lasso_coef_df$Variable, "eye_detection")))
  test_data_selected <- test_data %>% select(all_of(c(lasso_coef_df$Variable, "eye_detection")))
  

```



```{r random-forest}

set.seed(1754)
rf_model <- randomForest(eye_detection ~ ., data = train_data_selected, 
                           ntree = 500, mtry = 12, maxdepth = 5, nodesize = 20, 
                           sampsize = ceiling(0.7 * nrow(train_data_selected)), 
                           classwt = c(0.5, 0.5), 
                           importance = TRUE)

print(rf_model)

train_predictions_rf <- predict(rf_model, newdata = train_data_selected, type = "class")
train_accuracy_rf <- mean(train_predictions_rf == train_data_selected$eye_detection)
cat("Random Forest Training Accuracy:", train_accuracy_rf, "\n")

test_predictions_rf <- predict(rf_model, newdata = test_data_selected, type = "class")
test_accuracy_rf <- mean(test_predictions_rf == test_data_selected$eye_detection)
cat("Random Forest Test Accuracy:", test_accuracy_rf, "\n")

  
# Calculate ROC and AUC
test_prob_rf <- predict(rf_model, newdata = test_data_selected, type = "prob")
# Make sure the order of the categories corresponding to the probabilities is consistent with eye_detection
cat("Levels of test_predictions_rf:", levels(test_predictions_rf), "\n")
cat("Column names of test_prob_rf:", colnames(test_prob_rf), "\n")
# Probability of extracting the positive class (1)
if ("1" %in% colnames(test_prob_rf)) {
  test_prob_rf_class1 <- test_prob_rf[, "1"]
} else {
  stop("Positive class probability '1' not found in Random Forest predictions. Check class levels.")
}

# Convert eye_detection to a numeric vector (0 and 1)
response_rf <- as.numeric(test_data_selected$eye_detection) - 1
cat("Structure of response_rf:", str(response_rf), "\n")

# Calculate ROC and AUC
roc_rf <- roc(response_rf, test_prob_rf_class1)
auc_rf <- auc(roc_rf)
cat("Random Forest AUC:", auc_rf, "\n")
  
# Confusion matrix
conf_matrix_rf <- confusionMatrix(test_predictions_rf, test_data_selected$eye_detection)
print(conf_matrix_rf)
  
# Get variable importance and filter the top 30 features
var_importance <- importance(rf_model)
var_importance_df <- data.frame(Variable = rownames(var_importance), 
                                  Importance = var_importance[, "MeanDecreaseGini"]) %>%
    arrange(desc(Importance)) %>%
    head(30)

# Plot the importance of the top 10 features
var_importance_top10 <- var_importance_df %>% head(10)
ggplot(var_importance_top10, aes(x = reorder(Variable, Importance), y = Importance)) +
    geom_bar(stat = "identity", fill = "skyblue") +
    coord_flip() +
    theme_minimal() +
    labs(title = "Top 10 Variable Importance (MeanDecreaseGini)", 
         x = "Variable", y = "Importance") +
    theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 1))

knitr::kable(var_importance_df, caption = "Top 30 Variable Importance")

# Update the dataset to keep only the selected features
selected_features <- var_importance_df$Variable
train_data_selected <- train_data_selected %>% dplyr::select(all_of(c(selected_features, "eye_detection")))
test_data_selected <- test_data_selected %>% dplyr::select(all_of(c(selected_features, "eye_detection")))

```

## 8. Logistic model


```{r}
# Ensuring Data Integrity
cat("Checking for NA in train_data_selected:\n")
print(any(is.na(train_data_selected)))
cat("Checking for NA in test_data_selected:\n")
print(any(is.na(test_data_selected)))

# Training a Logistic Regression Model
logistic_model <- glm(eye_detection ~ ., data = train_data_selected, family = "binomial")

summary_logistic <- summary(logistic_model)
print(summary_logistic)

# Training set prediction and accuracy
train_prob_logistic <- predict(logistic_model, newdata = train_data_selected, type = "response")
train_predicted_classes_logistic <- ifelse(train_prob_logistic > 0.5, 1, 0) %>% as.factor()
levels(train_predicted_classes_logistic) <- levels(train_data_selected$eye_detection)
train_accuracy_logistic <- mean(train_predicted_classes_logistic == train_data_selected$eye_detection)
cat("Logistic Regression Training Accuracy:", train_accuracy_logistic, "\n")

# Testing set prediction and accuracy
test_prob_logistic <- predict(logistic_model, newdata = test_data_selected, type = "response")
test_predicted_classes_logistic <- ifelse(test_prob_logistic > 0.5, 1, 0) %>% as.factor()
levels(test_predicted_classes_logistic) <- levels(test_data_selected$eye_detection)
test_accuracy_logistic <- mean(test_predicted_classes_logistic == test_data_selected$eye_detection)
cat("Logistic Regression Test Accuracy:", test_accuracy_logistic, "\n")

# Confusion matrix
conf_matrix_logistic <- confusionMatrix(test_predicted_classes_logistic, test_data_selected$eye_detection)
print(conf_matrix_logistic)

# ROC and AUC
roc_logistic <- roc(as.numeric(test_data_selected$eye_detection) - 1, test_prob_logistic)
auc_logistic <- auc(roc_logistic)
cat("Logistic Regression AUC:", auc_logistic, "\n")
```




## 9. SVM Model with Hyperparameter Tuning


```{r}
# Checking Data Integrity
cat("Checking for NA in train_data_selected:\n")
print(any(is.na(train_data_selected)))
cat("Checking for NA in test_data_selected:\n")
print(any(is.na(test_data_selected)))

# Extracting features and labels
X_train <- as.matrix(train_data_selected %>% dplyr::select(-eye_detection))
y_train <- train_data_selected$eye_detection
X_test <- as.matrix(test_data_selected %>% dplyr::select(-eye_detection))
y_test <- test_data_selected$eye_detection

# Scale the data
means <- colMeans(X_train)
sds <- apply(X_train, 2, sd)
sds[sds == 0] <- 1  
X_train <- scale(X_train, center = means, scale = sds)
X_test <- scale(X_test, center = means, scale = sds)
# Train the model
set.seed(1754)
svm_model <- e1071::svm(x = X_train, y = y_train, 
                          kernel = "radial", 
                          cost = 1, 
                          gamma = 0.1, 
                          probability = TRUE, 
                          class.weights = c("0" = 1, "1" = 1))


print(summary(svm_model))

# Predict training and test sets
train_predictions_svm <- predict(svm_model, newdata = X_train)
train_accuracy_svm <- mean(train_predictions_svm == y_train)
cat("SVM Training Accuracy:", train_accuracy_svm, "\n")

test_predictions_svm <- predict(svm_model, newdata = X_test)
test_accuracy_svm <- mean(test_predictions_svm == y_test)
cat("SVM Test Accuracy:", test_accuracy_svm, "\n")

# Confusion matrix
confusion_matrix_svm <- confusionMatrix(test_predictions_svm, y_test)
print(confusion_matrix_svm)

# ROC and AUC

test_prob_svm <- predict(svm_model, newdata = X_test, probability = TRUE)
cat("SVM Probability Output (Full Structure):\n")
str(test_prob_svm)

test_prob_svm <- attr(test_prob_svm, "probabilities")
cat("SVM Probabilities (First 5 Rows):\n")
print(head(test_prob_svm))

cat("Levels of y_test:", levels(y_test), "\n")
cat("Column names of test_prob_svm:", colnames(test_prob_svm), "\n")

if ("1" %in% colnames(test_prob_svm)) {
    svm_probs <- test_prob_svm[, "1"]
    cat("Structure of svm_probs:\n")
    str(svm_probs)
    if (any(is.na(svm_probs))) {
      stop("SVM probabilities contain NA values.")
    }
  } else {
    stop("Class '1' probability not found in SVM predictions.")
  }

# Make sure the response is a binary value (0 and 1)
response_svm <- as.numeric(y_test) - 1
cat("Structure of response_svm:\n")
str(response_svm)

# Calculating ROC and AUC
roc_svm <- roc(response_svm, svm_probs)
auc_svm <- auc(roc_svm)
cat("SVM AUC:", auc_svm, "\n")
```

## 10. MLP Model

```{r mlp-model}

# Tuning parameters
set.seed(1754)
trControl <- trainControl(method = "cv", number = 5)
tuneGrid <- expand.grid(size = c(8, 10, 12), 
                          decay = c(0.5, 1, 3))

mlp_train_time <- system.time({
    mlp_model <- train(eye_detection ~ ., data = train_data_selected, 
                       method = "nnet", 
                       trControl = trControl, 
                       tuneGrid = tuneGrid, 
                       preProcess = c("center", "scale"),
                       maxit = 1000, 
                       trace = FALSE,
                       class.weights = c("0" = 1, "1" = 1))
  })


print(mlp_model)

# Predict training and test sets
train_predictions_mlp <- predict(mlp_model, newdata = train_data_selected)
train_accuracy_mlp <- mean(train_predictions_mlp == train_data_selected$eye_detection)
cat("MLP Training Accuracy:", train_accuracy_mlp, "\n")

# Record prediction time
mlp_predict_time <- system.time({
  test_predictions_mlp <- predict(mlp_model, newdata = test_data_selected)
  })
test_accuracy_mlp <- mean(test_predictions_mlp == test_data_selected$eye_detection)
cat("MLP Test Accuracy:", test_accuracy_mlp, "\n")

# confusion matrix
confusion_matrix_mlp <- confusionMatrix(test_predictions_mlp, test_data_selected$eye_detection)
print(confusion_matrix_mlp)

# ROC and AUC
test_prob_mlp <- predict(mlp_model, newdata = test_data_selected, type = "prob")
roc_mlp <- roc(as.numeric(test_data_selected$eye_detection) - 1, test_prob_mlp[,2])
auc_mlp <- auc(roc_mlp)
cat("MLP AUC:", auc_mlp, "\n")

```


## 11. kNN Model with Parameter Tuning

```{r knn-model}
# Tuning the model
set.seed(1754)
trControl <- trainControl(method = "cv", number = 5)
tuneGrid <- expand.grid(k = c(3, 5, 7, 9, 11))

# Train the model
knn_model <- train(eye_detection ~ ., data = train_data_selected,
                     method = "knn",
                     trControl = trControl,
                     tuneGrid = tuneGrid,
                     preProcess = c("center", "scale"))

print(knn_model)

# Predict training and test sets
train_predictions_knn <- predict(knn_model, newdata = train_data_selected)
train_accuracy_knn <- mean(train_predictions_knn == train_data_selected$eye_detection)
cat("kNN Training Accuracy:", train_accuracy_knn, "\n")

test_predictions_knn <- predict(knn_model, newdata = test_data_selected)
test_accuracy_knn <- mean(test_predictions_knn == test_data_selected$eye_detection)
cat("kNN Test Accuracy:", test_accuracy_knn, "\n")

# Confusion matrix
confusion_matrix_knn <- confusionMatrix(test_predictions_knn, test_data_selected$eye_detection)
print(confusion_matrix_knn)

# ROC and AUC
test_prob_knn <- predict(knn_model, newdata = test_data_selected, type = "prob")
cat("kNN Probability Output (First 5 Rows):\n")
print(head(test_prob_knn))

cat("Levels of eye_detection:", levels(test_data_selected$eye_detection), "\n")
cat("Column names of test_prob_knn:", colnames(test_prob_knn), "\n")

# Dynamically extract positive class probability (Class1)
positive_class <- levels(test_data_selected$eye_detection)[2]  # Class1
if (positive_class %in% colnames(test_prob_knn)) {
    knn_probs <- test_prob_knn[, positive_class]
  } else {
    cat("Available column names in test_prob_knn:", colnames(test_prob_knn), "\n")
    stop("Positive class probability not found in kNN predictions. Check column names above.")
  }

response_knn <- as.numeric(test_data_selected$eye_detection == positive_class)
roc_knn <- roc(response_knn, knn_probs)
auc_knn <- auc(roc_knn)
cat("kNN AUC:", auc_knn, "\n")


```

## 12. Model comparison and statistical tests

### 12.1 ROC Curves

```{r roc-curves, fig.width=6, fig.height=4}
plot(roc_rf, col = "purple", main = "ROC Curves for All Models")
plot(roc_logistic, col = "orange", add = TRUE)
plot(roc_svm, col = "blue", add = TRUE)
plot(roc_mlp, col = "red", add = TRUE)
plot(roc_knn, col = "green", add = TRUE)
legend("bottomright", legend = c(paste("Random Forest (AUC =", round(auc_rf, 3), ")"),
                                 paste("Logistic Regression (AUC =", round(auc_logistic, 3), ")"),
                                 paste("SVM (AUC =", round(auc_svm, 3), ")"),
                                 paste("MLP (AUC =", round(auc_mlp, 3), ")"),
                                 paste("kNN (AUC =", round(auc_knn, 3), ")")),
       col = c("purple", "orange", "blue", "red", "green"), lwd = 2)
```

### 12.2 McNemar's 檢驗

```{r mcnemar-test}
# Build a vector of correct/wrong predictions
rf_correct <- test_predictions_rf == test_data_selected$eye_detection
logistic_correct <- test_predicted_classes_logistic == test_data_selected$eye_detection
svm_correct <- test_predictions_svm == test_data_selected$eye_detection
mlp_correct <- test_predictions_mlp == test_data_selected$eye_detection
knn_correct <- test_predictions_knn == test_data_selected$eye_detection

# McNemar's test
mcnemar_rf_logistic <- mcnemar.test(rf_correct, logistic_correct)
mcnemar_rf_svm <- mcnemar.test(rf_correct, svm_correct)
mcnemar_rf_mlp <- mcnemar.test(rf_correct, mlp_correct)
mcnemar_rf_knn <- mcnemar.test(rf_correct, knn_correct)
mcnemar_logistic_svm <- mcnemar.test(logistic_correct, svm_correct)
mcnemar_logistic_mlp <- mcnemar.test(logistic_correct, mlp_correct)
mcnemar_logistic_knn <- mcnemar.test(logistic_correct, knn_correct)
mcnemar_svm_mlp <- mcnemar.test(svm_correct, mlp_correct)
mcnemar_svm_knn <- mcnemar.test(svm_correct, knn_correct)
mcnemar_mlp_knn <- mcnemar.test(mlp_correct, knn_correct)

# Results
cat("McNemar's Test (Random Forest vs Logistic Regression): p-value =", mcnemar_rf_logistic$p.value, "\n")
cat("McNemar's Test (Random Forest vs SVM): p-value =", mcnemar_rf_svm$p.value, "\n")
cat("McNemar's Test (Random Forest vs MLP): p-value =", mcnemar_rf_mlp$p.value, "\n")
cat("McNemar's Test (Random Forest vs kNN): p-value =", mcnemar_rf_knn$p.value, "\n")
cat("McNemar's Test (Logistic Regression vs SVM): p-value =", mcnemar_logistic_svm$p.value, "\n")
cat("McNemar's Test (Logistic Regression vs MLP): p-value =", mcnemar_logistic_mlp$p.value, "\n")
cat("McNemar's Test (Logistic Regression vs kNN): p-value =", mcnemar_logistic_knn$p.value, "\n")
cat("McNemar's Test (SVM vs MLP): p-value =", mcnemar_svm_mlp$p.value, "\n")
cat("McNemar's Test (SVM vs kNN): p-value =", mcnemar_svm_knn$p.value, "\n")
cat("McNemar's Test (MLP vs kNN): p-value =", mcnemar_mlp_knn$p.value, "\n")
```

## 13. 模型比較和學習曲線
```{r}
# Model comparison table
model_comparison <- data.frame(
  Model = c("Random Forest", "Logistic Regression", "SVM", "MLP", "kNN"),
  Train_Accuracy = c(train_accuracy_rf, train_accuracy_logistic, train_accuracy_svm, train_accuracy_mlp, train_accuracy_knn),
  Test_Accuracy = c(test_accuracy_rf, test_accuracy_logistic, test_accuracy_svm, test_accuracy_mlp, test_accuracy_knn),
  AUC = c(auc_rf, auc_logistic, auc_svm, auc_mlp, auc_knn),
  Precision = c(conf_matrix_rf$byClass["Pos Pred Value"], conf_matrix_logistic$byClass["Pos Pred Value"], 
                confusion_matrix_svm$byClass["Pos Pred Value"], confusion_matrix_mlp$byClass["Pos Pred Value"], 
                confusion_matrix_knn$byClass["Pos Pred Value"]),
  Recall = c(conf_matrix_rf$byClass["Sensitivity"], conf_matrix_logistic$byClass["Sensitivity"], 
             confusion_matrix_svm$byClass["Sensitivity"], confusion_matrix_mlp$byClass["Sensitivity"], 
             confusion_matrix_knn$byClass["Sensitivity"]),
  F1_Score = c(conf_matrix_rf$byClass["F1"], conf_matrix_logistic$byClass["F1"], 
               confusion_matrix_svm$byClass["F1"], confusion_matrix_mlp$byClass["F1"], 
               confusion_matrix_knn$byClass["F1"])
)

knitr::kable(model_comparison, caption = "Comparison of Model Performance Metrics", digits = 3)
```


## 13.2 learning Curves

```{r learning-curves, fig.width=8, fig.height=6}
# Define learning curve function
learning_curve <- function(model_type, train_data, test_data) {
  train_sizes <- seq(0.2, 1.0, by = 0.2)
  results <- data.frame(Train_Size = numeric(), Train_Accuracy = numeric(), Test_Accuracy = numeric())
  
  for (size in train_sizes) {
    set.seed(1754)
    subset_index <- createDataPartition(train_data$eye_detection, p = size, list = FALSE)
    subset_train <- train_data[subset_index, ]
    
    if (model_type == "rf") {
      model <- randomForest(eye_detection ~ ., data = subset_train, 
                            ntree = 500, mtry = 12, maxdepth = 5, nodesize = 20, 
                            sampsize = ceiling(0.7 * nrow(subset_train)), 
                            classwt = c(0.5, 0.5))
    } else if (model_type == "logistic") {
      model <- glm(eye_detection ~ ., data = subset_train, family = "binomial")
      train_pred <- predict(model, newdata = subset_train, type = "response")
      train_pred <- ifelse(train_pred > 0.5, 1, 0) %>% as.factor()
      levels(train_pred) <- levels(subset_train$eye_detection)
      test_pred <- predict(model, newdata = test_data, type = "response")
      test_pred <- ifelse(test_pred > 0.5, 1, 0) %>% as.factor()
      levels(test_pred) <- levels(test_data$eye_detection)
    } else if (model_type == "svm") {
      model <- svm(eye_detection ~ ., data = subset_train, 
                   kernel = "radial", 
                   gamma = 0.1, 
                   cost = 1, 
                   probability = TRUE)
    } else if (model_type == "mlp") {
      model <- train(eye_detection ~ ., data = subset_train, 
                     method = "nnet", 
                     trControl = trainControl(method = "cv", number = 5), 
                     tuneGrid = expand.grid(size = 12, decay = 0.5), 
                     preProcess = c("center", "scale"),
                     maxit = 1000, trace = FALSE)
    } else if (model_type == "knn") {
      model <- train(eye_detection ~ ., data = subset_train,
                     method = "knn",
                     trControl = trainControl(method = "cv", number = 5),
                     tuneGrid = expand.grid(k = 5),
                     preProcess = c("center", "scale"))
    }
    
    if (model_type == "logistic") {
      train_acc <- mean(train_pred == subset_train$eye_detection)
      test_acc <- mean(test_pred == test_data$eye_detection)
    } else {
      train_pred <- predict(model, newdata = subset_train)
      test_pred <- predict(model, newdata = test_data)
      train_acc <- mean(train_pred == subset_train$eye_detection)
      test_acc <- mean(test_pred == test_data$eye_detection)
    }
    
    results <- rbind(results, data.frame(Train_Size = size * nrow(train_data), 
                                         Train_Accuracy = train_acc, 
                                         Test_Accuracy = test_acc))
  }
  return(results)
}

# Calculate learning curves
rf_learning <- learning_curve("rf", train_data_selected, test_data_selected)
logistic_learning <- learning_curve("logistic", train_data_selected, test_data_selected)
svm_learning <- learning_curve("svm", train_data_selected, test_data_selected)
mlp_learning <- learning_curve("mlp", train_data_selected, test_data_selected)
knn_learning <- learning_curve("knn", train_data_selected, test_data_selected)

# Plot
learning_data <- rbind(
  data.frame(rf_learning, Model = "Random Forest", Type = "Train", Accuracy = rf_learning$Train_Accuracy),
  data.frame(rf_learning, Model = "Random Forest", Type = "Test", Accuracy = rf_learning$Test_Accuracy),
  data.frame(logistic_learning, Model = "Logistic Regression", Type = "Train", Accuracy = logistic_learning$Train_Accuracy),
  data.frame(logistic_learning, Model = "Logistic Regression", Type = "Test", Accuracy = logistic_learning$Test_Accuracy),
  data.frame(svm_learning, Model = "SVM", Type = "Train", Accuracy = svm_learning$Train_Accuracy),
  data.frame(svm_learning, Model = "SVM", Type = "Test", Accuracy = svm_learning$Test_Accuracy),
  data.frame(mlp_learning, Model = "MLP", Type = "Train", Accuracy = mlp_learning$Train_Accuracy),
  data.frame(mlp_learning, Model = "MLP", Type = "Test", Accuracy = mlp_learning$Test_Accuracy),
  data.frame(knn_learning, Model = "kNN", Type = "Train", Accuracy = knn_learning$Train_Accuracy),
  data.frame(knn_learning, Model = "kNN", Type = "Test", Accuracy = knn_learning$Test_Accuracy)
)

ggplot(learning_data, aes(x = Train_Size, y = Accuracy, color = Model, linetype = Type)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(title = "Learning Curves for All Models", x = "Training Sample Size", y = "Accuracy") +
  scale_color_manual(values = c("Random Forest" = "purple", "Logistic Regression" = "orange", 
                                "SVM" = "blue", "MLP" = "red", "kNN" = "green"))
```




