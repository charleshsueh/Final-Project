---
title: "Eye State Classification Analysis"
subtitle: "⚔ Final Project"
author: "Chun-Chien(Charles) Hsueh"
institute: "Rutgers University"
output:
  xaringan::moon_reader:
    css: ["rutgers", "rutgers-fonts"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

# Outline

- **Dataset Introduction**
- **Data Preprocessing**
- **Variable Selection**
- **Modeling**
- **Model Diagnostics**
- **Conclusions**
- **What to do next...**

---

# Dataset Introduction
## Background and Variables

- **Dataset Origin**: The `eye_state.arff` dataset is derived from EEG recordings collected to study eye state detection. It was created to support research on brain-computer interfaces (BCI) and human-computer interaction
- **Context**: The data captures EEG signals during different eye states (open or closed), typically recorded using multiple electrodes placed on the scalp
- **Variables**:
  - `Channel1` to `Channel14`: Represent EEG signals from 14 distinct scalp electrodes, measuring voltage fluctuations due to brain activity. Units are in microvolts (µV), standardized in the analysis
  - `eyeDetection`: Binary target variable indicating eye state (0 = Closed, 1 = Open), derived from manual or automated labeling of EEG sessions

---

# Data Preprocessing

- **Tidying**:
  - Converted `eyeDetection` to factor (`Closed`/`Open`)
  - Created numeric version (`eyeDetection_numeric`) for LASSO
- **Standardization**: Scaled predictor variables (Channels 1–14)
- **Verification**: No missing values 

---

# Variable Selection

- **Method**: LASSO regression
- **Selected Variables**:
  - Channels 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 (all channels retained).
- **Visualization**:

```{r,, warning=FALSE, message=FALSE, echo=FALSE, out.width="50%"}
library(here)
knitr::include_graphics(here("photo","1.png"))
knitr::include_graphics(here("photo","2.png"))
```


---

# Modeling
## Logistic Regression


- **Model Fit**:
  - Significant predictors: Channels 1, 2, 3, 4, 5, 6, 7, 11, 12, 14 (p < 0.05)
  - Example: Channel 1 coefficient = 16.08 (positive impact), Channel 11 = -3.675 (negative impact)
- **Model Metrics**:
  - Null deviance: 20609 on 14979 df
  - Residual deviance: 19156 on 14965 df
  - AIC: 19186
- **Interpretation**:
  - Channels like 2, 4, 6 have strong negative effects on predicting "Open" state
  - Channels 1, 5, 7, 14 have strong positive effects

---

# Modeling
## Bayesian


- **Posterior Estimates**:
  - Channel 1: 16.24 (95% CI: 6.64, 25.50)
  - Channel 11: -0.37 (95% CI: -0.49, -0.25)
  - Channel 14: 30.59 (95% CI: 6.54, 54.59)
- **Convergence**:
  - All Rhat values = 1.00, indicating good convergence
  - Effective sample sizes (ESS) are high (e.g., 1399–2103)
  - AIC: 19186
- **Interpretation**:
  - Bayesian model confirms logistic regression findings with wider uncertainty intervals
  - Channels 4 and 6 have the largest negative effects, consistent with logistic model


---

# Model Diagnostics

```{r,, warning=FALSE, message=FALSE, echo=FALSE, out.width="50%"}
library(here)
knitr::include_graphics(here("photo","3.png"))
knitr::include_graphics(here("photo","4.png"))
```

- **Comment**:
  - The model may not fully capture nonlinear relationships in the data, and the linearity assumption of the model may not be entirely appropriate
  
---

# Conclusion

- **Effectiveness**:
  - LASSO retained all 14 channels, suggesting all EEG signals are relevant
  - Both logistic and Bayesian models agree on key predictors (e.g., Channels 1, 2, 4, 6, 11, 14)
- **Model Fit**:
  - Residual deviance reduced significantly (20609 to 19156), but residual patterns suggest room for improvement
- **Practical Implication**:
  - Model can predict eye state with reasonable accuracy but may miss non-linear relationships

---

# What to do next...


- **Non-Linear Modeling**

- **Feature Engineering**:
  - Explore interactions between channels or frequency-domain features of EEG data
- **Model testing**:
  - Divide the data into training sets and test sets to further evaluate the generalization ability of the model
- **Model Comparison**:
  - Compare with other classifiers (e.g., random forests, SVM) to improve performance

---
class: center, middle

# Thank you!!!







